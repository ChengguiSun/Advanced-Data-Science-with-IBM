{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SparkML_SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOeRg5z/2xNjwLZayAF534g"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxF1Z4QadpsI",
        "colab_type": "text"
      },
      "source": [
        "# Install Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irTGeJvU4pI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zf05GSbAkHcO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "outputId": "e6e4ee4d-f4c3-4504-c232-f48b3efea43d"
      },
      "source": [
        "!pip install pyspark==2.4.5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark==2.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 57kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 40.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=38c87a6dbf8a0e1eb8207c42f5bdc69ef58af675aedfdc448028b81bed5967b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH8c_Y9njMJ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9bb03b55-0229-4d7c-c2b4-e385c850b7fb"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data  spark-3.0.0-bin-hadoop2.7\tspark-3.0.0-bin-hadoop2.7.tgz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWGiaP0Sjs0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I86xC0GgkG_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Onm9GcD5Zda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    from pyspark import SparkContext, SparkConf\n",
        "except ImportError as e:\n",
        "    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg1sgY0d3Ygw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
        "\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .getOrCreate()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9OwwEXHiMmq",
        "colab_type": "text"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpvT6s-KeD0e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "3a28780d-a3a9-40ab-d04b-1a2671090741"
      },
      "source": [
        "# download the file containing the data in PARQUET format\n",
        "!wget https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
        "    \n",
        "# create a dataframe out of it\n",
        "df = spark.read.parquet('hmp.parquet')\n",
        "\n",
        "# register a corresponding query table\n",
        "df.createOrReplaceTempView('df')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-19 17:29:02--  https://github.com/IBM/coursera/raw/master/hmp.parquet\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet [following]\n",
            "--2020-07-19 17:29:02--  https://github.com/IBM/skillsnetwork/raw/master/hmp.parquet\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet [following]\n",
            "--2020-07-19 17:29:03--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/hmp.parquet\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 932997 (911K) [application/octet-stream]\n",
            "Saving to: ‘hmp.parquet’\n",
            "\n",
            "hmp.parquet         100%[===================>] 911.13K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-07-19 17:29:05 (7.11 MB/s) - ‘hmp.parquet’ saved [932997/932997]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3ad2F1ks8qR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "outputId": "1f245b09-5875-4af2-a4eb-935976e6bf71"
      },
      "source": [
        "df.show(5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+--------------------+-----------+\n",
            "|  x|  y|  z|              source|      class|\n",
            "+---+---+---+--------------------+-----------+\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 49| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 22| 52| 35|Accelerometer-201...|Brush_teeth|\n",
            "| 21| 52| 34|Accelerometer-201...|Brush_teeth|\n",
            "+---+---+---+--------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr4GnxAM0SXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = df.randomSplit([0.8, 0.2])\n",
        "df_train = splits[0]\n",
        "df_test = splits[1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhVKoAEY0SbK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "\n",
        "indexer = StringIndexer(inputCol=\"class\", outputCol=\"label\")\n",
        "encoder = OneHotEncoder(inputCol=\"label\", outputCol=\"labelVec\")\n",
        "vectorAssembler = VectorAssembler(inputCols=[\"x\",\"y\",\"z\"],\n",
        "                                  outputCol=\"features\")\n",
        "normalizer = MinMaxScaler(inputCol=\"features\", outputCol=\"features_norm\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZnaOiTS0SfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "\n",
        "lsvc = LinearSVC(maxIter=10, regParam=0.1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyqREbft0SiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.createOrReplaceTempView('df')\n",
        "df_two_class = spark.sql(\"select * from df where class in ('Use_telephone','Standup_chair')\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdu-oyMT0Slt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "splits = df_two_class.randomSplit([0.8, 0.2])\n",
        "df_train = splits[0]\n",
        "df_test = splits[1]"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqn6M5pBJL8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "786c883d-41ff-4748-966f-58909b7df89b"
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Row(x=0, y=30, z=24, source='Accelerometer-2011-06-01-14-40-19-standup_chair-f1.txt', class='Standup_chair')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nj3nDio0Spj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "pipeline = Pipeline(stages=[indexer, encoder, vectorAssembler, normalizer,lsvc])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "598jPq2l0SsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = pipeline.fit(df_train)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFy73VV50Syp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.transform(df_train)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2YDO_b70SwS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c9bc246-c71c-4c8e-800c-0871eecd6c17"
      },
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# Evaluate model\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "evaluator.evaluate(prediction)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9385735636757285"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n10Qca0W0sX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prediction = model.transform(df_test)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyeX8Q8G0sbd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f678c473-828b-4189-97c8-7c7121859ae7"
      },
      "source": [
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
        "evaluator.evaluate(prediction)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9398484822061777"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    }
  ]
}